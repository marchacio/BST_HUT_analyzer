import os
import csv
import argparse
import time
import sys

# Fix relative imports. See https://stackoverflow.com/a/16985066
SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))
sys.path.append(os.path.dirname(SCRIPT_DIR))

from src.utils.clone_repo import clone_repo
from legacy.code_analyzer import code_analyzer_per_commit
from src.utils.log import init_logging, log

# --- Global Configuration ---
file_extension = 'js'
sast_analyzer = True
secret_analyzer = True
cyclomatic_analyzer = True
text_metrics_analyzer = True

def _create_csv_file(
    csv_file_path: str,
    tags: list,
    analyze_all_file: bool = False,
):
    """Creates and populates a CSV file with analysis data for each Git tag."""
    with open(csv_file_path, mode='w', newline='', encoding='utf-8') as csv_file:
        csv_writer = csv.writer(csv_file)

        header = [
            "Tag", "Date", "Commit_Author", "Type",
            '#LoC', '#Files',
            "Function_Count", "Async_Function_Count", "Class_Count",
            "Entropy", "Dependencies_Count",
        ]

        # Add headers for optional analyzers
        if sast_analyzer:
            header += [
                'SAST_findings_count', 'SAST_findings_high_count', 'SAST_findings_high',
                'SAST_findings_medium_count', 'SAST_findings_medium', 'SAST_findings_low_count',
                'SAST_findings_low',
            ]
        if secret_analyzer:
            header += [
                'Secret_Findings_Count', 'Secret_Findings_High_Count',
                'Secret_Findings_Medium_Count', 'Secret_Findings_Low_Count'
            ]
        if cyclomatic_analyzer:
            header += [
                'CC_Function_Count', 'CC_Function_Average', 'CC_Module_Count',
                'CC_Module_Average', 'CC_Method_Count', 'CC_Method_Average',
            ]
        if text_metrics_analyzer:
            header += [
                'Longest_Line_Length', 'Longest_Line_File',
                'Relative_Higher_Blank_Space_Ratio', 'Absolute_Blank_Space_Ratio',
                'Total_Chars', 'Total_Blank_Spaces',
            ]

        csv_writer.writerow(header)

        previous_tag_name = None
        tag_length = len(tags)

        for i, tag in enumerate(tags):
            log(f"\rAnalyzing tag {tag.name} ({i+1}/{tag_length})... ")
            commit = tag.commit

            code_data = code_analyzer_per_commit(
                commit,
                file_extension=file_extension,
                analyze_all_file=analyze_all_file,
                sast_analyzer=sast_analyzer,
                secret_analyzer=secret_analyzer,
                cyclomatic_complexity_analyzer=cyclomatic_analyzer,
                text_metrics_analyzer=text_metrics_analyzer,
            )

            # Determine tag type (major, minor, patch) based on versioning
            tag_type = "initial"
            if previous_tag_name:
                try:
                    current_parts = tag.name.split('.')
                    prev_parts = previous_tag_name.split('.')
                    if current_parts[0] != prev_parts[0]:
                        tag_type = "major"
                    elif len(current_parts) > 1 and len(prev_parts) > 1 and current_parts[1] != prev_parts[1]:
                        tag_type = "minor"
                    else:
                        tag_type = "patch"
                except IndexError:
                    tag_type = "unknown" # Handle non-semantic versioning tags

            data_row = [
                tag.name,
                commit.committed_datetime.strftime("%Y-%m-%d %H:%M:%S"),
                commit.author.name,
                tag_type,
                code_data.get('total_loc', 0),
                code_data.get('total_files', 0),
                code_data.get('function_count', 0),
                code_data.get('async_function_count', 0),
                code_data.get('class_count', 0),
                code_data.get('entropy', 0),
                code_data.get('dependencies_count', 0),
            ]

            if sast_analyzer:
                data_row.extend([
                    code_data.get('sast_findings_count', 0),
                    code_data.get('sast_findings_high_count', 0),
                    code_data.get('sast_findings_high', ''),
                    code_data.get('sast_findings_medium_count', 0),
                    code_data.get('sast_findings_medium', ''),
                    code_data.get('sast_findings_low_count', 0),
                    code_data.get('sast_findings_low', ''),
                ])
            if secret_analyzer:
                data_row.extend([
                    code_data.get('secret_findings_count', 0),
                    code_data.get('secret_findings_high_count', 0),
                    code_data.get('secret_findings_medium_count', 0),
                    code_data.get('secret_findings_low_count', 0)
                ])
            if cyclomatic_analyzer:
                data_row.extend([
                    code_data.get('cc_function_count', 0),
                    code_data.get('cc_function_average', 0),
                    code_data.get('cc_module_count', 0),
                    code_data.get('cc_module_average', 0),
                    code_data.get('cc_method_count', 0),
                    code_data.get('cc_method_average', 0),
                ])
            if text_metrics_analyzer:
                data_row.extend([
                    code_data.get('longest_line_length', 0),
                    code_data.get('longest_line_file', ''),
                    code_data.get('relative_higher_blank_space_ratio', 0),
                    code_data.get('blank_space_ratio', 0),
                    code_data.get('total_chars', 0),
                    code_data.get('total_blank_spaces', 0),
                ])

            csv_writer.writerow(data_row)
            previous_tag_name = tag.name

    log(f"\n\nAnalysis complete. Results have been saved to {csv_file_path}.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="""Analyze a Git repository for code and security metrics.
The analysis includes SAST (Static Application Security Testing), secret detection, cyclomatic complexity, and more.
The results are saved in CSV files for further analysis.""",
        usage="python tool.py <repo_url> [options]",
    )
    parser.add_argument("repo_url", type=str, help="The URL of the Git repository to analyze.")
    parser.add_argument("--file-extension", "-e", type=str, default="js", help="The file extension to analyze. Default is 'js'.")
    parser.add_argument("--analyze-all-file", action="store_true", help="Run the script twice: once for the specified file extension and once for all files.")
    parser.add_argument("--no-sast", action="store_true", help="Disable SAST analyzer.")
    parser.add_argument("--no-secret", action="store_true", help="Disable secret analyzer.")
    parser.add_argument("--no-cyclomatic", action="store_true", help="Disable cyclomatic complexity analyzer.")
    parser.add_argument("--no-text-metrics", action="store_true", help="Disable text metrics analyzer.")
    parser.add_argument("--no-log", action="store_true", help="Disable saving logs to a file.")
    
    args = parser.parse_args()

    repo_url = args.repo_url
    file_extension = args.file_extension
    analyze_all_file = args.analyze_all_file
    sast_analyzer = not args.no_sast
    secret_analyzer = not args.no_secret
    cyclomatic_analyzer = not args.no_cyclomatic
    text_metrics_analyzer = not args.no_text_metrics

    # Clone the repository
    repo = clone_repo(repo_url)
    repo_name = repo_url.rstrip('/').split('/')[-1].split('.git')[0]
    
    # Create the output directory for the repository's analytics
    base_path = os.path.join("analytics", repo_name)
    os.makedirs(base_path, exist_ok=True)
    
    # Initialize the logger
    init_logging(
        log_file=os.path.join(base_path, "log.txt"),
        save_file=(not args.no_log),
    )
    
    log('Script Arguments: ' + str(args))
    
    tags = sorted(repo.tags, key=lambda t: t.commit.authored_datetime)
    if len(tags) < 2:
        log("Not enough tags found to perform a meaningful analysis.")
        sys.exit(1)
              
    # Define paths for the output CSV files
    csv_ext_file_path = os.path.join(base_path, f'{file_extension}_code_analysis.csv')
    csv_all_file_path = os.path.join(base_path, 'all_code_analysis.csv')
    
    # Run analysis for the specified file extension
    start_time_ext = time.time()
    _create_csv_file(csv_ext_file_path, tags, analyze_all_file=False)
    end_time_ext = time.time()
    log(f"Execution time for '{file_extension}' file analysis: {end_time_ext - start_time_ext:.2f} seconds")

    # If requested, run analysis for all files
    if analyze_all_file:
        start_time_all = time.time()
        _create_csv_file(csv_all_file_path, tags, analyze_all_file=True)
        end_time_all = time.time()
        log(f"Execution time for all-file analysis: {end_time_all - start_time_all:.2f} seconds")